{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "439452c1-ba43-43d2-84c0-93d9a83a67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env \n",
    "from gymnasium.spaces import MultiDiscrete, Discrete\n",
    "# from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import numpy as np\n",
    "import torch \n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be1ecc41-03d3-4980-80ea-4a47143be029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 1, 21, 9, 6, 19, 6, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "hi#there,a                              \n",
      "{' ': 0, '#': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, ',': 28, '_': 29}\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers and vice versa\n",
    "chars = list(' #abcdefghijklmnopqrstuvwxyz,_')\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)} # abbrev for string to integer\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "# encoder: takes a string and returns a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s.ljust(40)]\n",
    "\n",
    "# decoder: takes a list of integers and returns a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "#unit test\n",
    "print(encode('hi#there,a'))\n",
    "print(decode(encode('hi#there,a')))\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93805859-eea8-444a-8c6f-a568f5d6d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HangmanServer(Env):\n",
    "    def __init__(self):\n",
    "        with open('words_250000_train.txt') as f:\n",
    "            self.test_words = f.read().split('\\n')\n",
    "        self.shuffle()\n",
    "        self.action_space = Discrete(26)\n",
    "        self.observation_space = MultiDiscrete([29]*40)\n",
    "        self.itr = 0\n",
    "        self.state = list('#'*len(self.test_words[self.itr])+',')\n",
    "        self.rem_guesses = 6\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.test_words)\n",
    "\n",
    "    def reset(self, seed = 0):\n",
    "        self.rem_guesses = 6\n",
    "        self.itr = self.itr + 1\n",
    "        self.state = list('#'*len(self.test_words[self.itr])+',')\n",
    "        return encode(\"\".join(self.state)), {}\n",
    "\n",
    "    def step(self, action = None, out = False):\n",
    "        if action == None:\n",
    "            action = chr(np.random.randint(97, 123))\n",
    "        else:\n",
    "            action = chr(action+97)\n",
    "        reward = 0\n",
    "        num_blanks = 0\n",
    "        done = 0\n",
    "        for itr in range(len(self.test_words[self.itr])):\n",
    "            if self.state[itr] == '#':\n",
    "                num_blanks = num_blanks + 1\n",
    "                if self.test_words[self.itr][itr] == action:\n",
    "                    self.state[itr] = action\n",
    "                    reward = 1\n",
    "                    num_blanks = num_blanks - 1\n",
    "        # if reward == 0:\n",
    "            # self.state.append(action)\n",
    "        \n",
    "\n",
    "        state = \"\".join(self.state)\n",
    "        if out:\n",
    "            print(state, action)\n",
    "        if reward==0:\n",
    "            self.rem_guesses -= 1\n",
    "            \n",
    "        if num_blanks == 0 or self.rem_guesses == 0:\n",
    "            done = 1\n",
    "            if num_blanks ==0: reward = 10\n",
    "            self.reset()\n",
    "        try:\n",
    "            encode(\"\".join(self.state))\n",
    "        except:\n",
    "            print(self.state, action)\n",
    "        return encode(\"\".join(self.state)), reward, done, done, {}\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "env = HangmanServer()\n",
    "\n",
    "\n",
    "\n",
    "# for itr in range(20):\n",
    "#     state, reward, done, _, _ = env.step(out = True)\n",
    "#     print(done)\n",
    "#     print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "321b6578-5fd0-4bae-8341-aaa9c8179006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "num_envs = 8\n",
    "vec_env = SubprocVecEnv([lambda: Monitor(HangmanServer(), log_dir) for i in range(num_envs)], start_method='fork')\n",
    "# for itr in range(2):\n",
    "#     state, reward, done, _ = vec_env.step([1]*2)\n",
    "#     print(done)\n",
    "#     print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b9bf97-d318-4a3d-a91f-c8c2e7304070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DQN agent\n",
    "# model = DQN(\"MlpPolicy\", vec_env, verbose=0, learning_rate=0.001, buffer_size=50000)\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch=dict(pi=[256, 128], vf=[256, 128]),\n",
    "    lstm_hidden_size=128,\n",
    "    n_lstm_layers=1,\n",
    "    shared_lstm=True,\n",
    "    enable_critic_lstm=False,\n",
    ")\n",
    "# model = RecurrentPPO(\n",
    "#     \"MlpLstmPolicy\",\n",
    "#     vec_env,\n",
    "#     policy_kwargs=policy_kwargs,\n",
    "#     learning_rate=1e-4,\n",
    "#     n_steps=256,\n",
    "#     batch_size=128,\n",
    "#     n_epochs=4,\n",
    "#     gamma=0.99,\n",
    "#     gae_lambda=0.95,\n",
    "#     clip_range=0.2,\n",
    "#     ent_coef=0.01,\n",
    "#     vf_coef=0.5,\n",
    "#     max_grad_norm=0.5,\n",
    "#     verbose=0)\n",
    "# # Train the agent\n",
    "# model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bd6e4-3afd-4471-a42a-63dc63dfebd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2a7ea75-7973-474a-8d92-7c9d37c88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=1e-5,\n",
    "    n_steps=256,\n",
    "    batch_size=128,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    verbose=0,\n",
    "    tensorboard_log=log_dir)\n",
    "\n",
    "# model.set_parameters(load_path_or_dict=\"ppo_recurrent\")\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: Monitor(HangmanServer(), log_dir)])  # Replace \"env\" with your environment\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir,\n",
    "    log_path=log_dir,\n",
    "    eval_freq=5000//num_envs,  # Evaluate every 2000 timesteps\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1,\n",
    "    n_eval_episodes = 100\n",
    "    \n",
    ")\n",
    "\n",
    "# 5. Create callback list (add more callbacks if needed)\n",
    "callbacks = CallbackList([eval_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3529aab5-c966-47b4-8795-4d6518db5fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/stable_baseline/lib/python3.11/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x149944850> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x149927450>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=0.74 +/- 0.48\n",
      "Episode length: 6.74 +/- 0.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=0.71 +/- 0.45\n",
      "Episode length: 6.71 +/- 0.45\n",
      "Eval num_timesteps=15000, episode_reward=0.66 +/- 0.47\n",
      "Episode length: 6.66 +/- 0.47\n",
      "Eval num_timesteps=20000, episode_reward=0.67 +/- 0.47\n",
      "Episode length: 6.67 +/- 0.47\n",
      "Eval num_timesteps=25000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=30000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=35000, episode_reward=0.61 +/- 0.49\n",
      "Episode length: 6.61 +/- 0.49\n",
      "Eval num_timesteps=40000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 6.60 +/- 0.49\n",
      "Eval num_timesteps=45000, episode_reward=0.72 +/- 0.45\n",
      "Episode length: 6.72 +/- 0.45\n",
      "Eval num_timesteps=50000, episode_reward=0.63 +/- 0.48\n",
      "Episode length: 6.63 +/- 0.48\n",
      "Eval num_timesteps=55000, episode_reward=0.67 +/- 0.47\n",
      "Episode length: 6.67 +/- 0.47\n",
      "Eval num_timesteps=60000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 6.60 +/- 0.49\n",
      "Eval num_timesteps=65000, episode_reward=0.73 +/- 0.44\n",
      "Episode length: 6.73 +/- 0.44\n",
      "Eval num_timesteps=70000, episode_reward=0.79 +/- 0.41\n",
      "Episode length: 6.79 +/- 0.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=80000, episode_reward=0.67 +/- 0.47\n",
      "Episode length: 6.67 +/- 0.47\n",
      "Eval num_timesteps=85000, episode_reward=0.59 +/- 0.49\n",
      "Episode length: 6.59 +/- 0.49\n",
      "Eval num_timesteps=90000, episode_reward=0.67 +/- 0.47\n",
      "Episode length: 6.67 +/- 0.47\n",
      "Eval num_timesteps=95000, episode_reward=0.59 +/- 0.49\n",
      "Episode length: 6.59 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.59 +/- 0.49\n",
      "Episode length: 6.59 +/- 0.49\n",
      "Eval num_timesteps=105000, episode_reward=0.64 +/- 0.48\n",
      "Episode length: 6.64 +/- 0.48\n",
      "Eval num_timesteps=110000, episode_reward=0.75 +/- 0.43\n",
      "Episode length: 6.75 +/- 0.43\n",
      "Eval num_timesteps=115000, episode_reward=0.68 +/- 0.47\n",
      "Episode length: 6.68 +/- 0.47\n",
      "Eval num_timesteps=120000, episode_reward=0.63 +/- 0.48\n",
      "Episode length: 6.63 +/- 0.48\n",
      "Eval num_timesteps=125000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=130000, episode_reward=0.73 +/- 0.44\n",
      "Episode length: 6.73 +/- 0.44\n",
      "Eval num_timesteps=135000, episode_reward=0.67 +/- 0.47\n",
      "Episode length: 6.67 +/- 0.47\n",
      "Eval num_timesteps=140000, episode_reward=0.65 +/- 0.48\n",
      "Episode length: 6.65 +/- 0.48\n",
      "Eval num_timesteps=145000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=150000, episode_reward=0.70 +/- 0.46\n",
      "Episode length: 6.70 +/- 0.46\n",
      "Eval num_timesteps=155000, episode_reward=0.61 +/- 0.49\n",
      "Episode length: 6.61 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.72 +/- 0.45\n",
      "Episode length: 6.72 +/- 0.45\n",
      "Eval num_timesteps=165000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 6.60 +/- 0.49\n",
      "Eval num_timesteps=170000, episode_reward=0.75 +/- 0.43\n",
      "Episode length: 6.75 +/- 0.43\n",
      "Eval num_timesteps=175000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 6.60 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=0.66 +/- 0.47\n",
      "Episode length: 6.66 +/- 0.47\n",
      "Eval num_timesteps=185000, episode_reward=0.59 +/- 0.49\n",
      "Episode length: 6.59 +/- 0.49\n",
      "Eval num_timesteps=190000, episode_reward=0.57 +/- 0.50\n",
      "Episode length: 6.57 +/- 0.50\n",
      "Eval num_timesteps=195000, episode_reward=0.58 +/- 0.49\n",
      "Episode length: 6.58 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=0.61 +/- 0.49\n",
      "Episode length: 6.61 +/- 0.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO at 0x149926c90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Train with callbacks\n",
    "model.learn(\n",
    "    total_timesteps=200000,\n",
    "    callback=callbacks,\n",
    "    tb_log_name=\"ppo_recurrent\"  # TensorBoard experiment name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ecc1474-375f-4dea-b9c9-73354ba1c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_recurrent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b45444e-a183-4e59-969c-342ffda70bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Rate:  0.092\n",
      "Episodic Reward:  6.078\n"
     ]
    }
   ],
   "source": [
    "env = HangmanServer()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "lstm_states = None\n",
    "episode_starts = True\n",
    "wins = 0\n",
    "games = 0\n",
    "tot_reward = 0\n",
    "ngames = 500\n",
    "while games<ngames:\n",
    "    # print(episode_starts, obs)\n",
    "\n",
    "    action, lstm_states = model.predict(\n",
    "                obs,\n",
    "                state=lstm_states,\n",
    "                episode_start=episode_starts,\n",
    "                deterministic=True\n",
    "            )\n",
    "    obs, reward, done, _, _ = env.step(action = action, out = False)\n",
    "   \n",
    "    episode_starts = done\n",
    "    tot_reward += reward\n",
    "    if done==1:\n",
    "        if reward==10:\n",
    "            wins+=1\n",
    "            games+=1\n",
    "        else:\n",
    "            games+=1\n",
    "\n",
    "\n",
    "print('Win Rate: ', wins/games)\n",
    "print('Episodic Reward: ', tot_reward/games)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "316acea0-3bcf-42cb-9c43-77aed2644fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"ppo_recurrent\")\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = RecurrentPPO.load(\"ppo_recurrent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbb8ce20-5817-4693-960b-508f384d3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=1000)\n",
    "# print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138edafb-ea93-4be2-b1df-1932ce4577e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
