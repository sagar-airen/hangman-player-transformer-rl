{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2391,
     "status": "ok",
     "timestamp": 1712946637859,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "CNpIclzccGEF"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sparse\n",
    "from scipy.sparse import linalg as splinalg\n",
    "from sys import argv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pl\n",
    "import warnings\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 6620,
     "status": "ok",
     "timestamp": 1712946644475,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "3eM9pu_EyWlY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn  import functional as F\n",
    "#use gpu if available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  2352046\n",
      "227200 ['aaa' 'aaaaaa' 'aaas' ... 'zwart' 'zweig' 'zwickau']\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('../words_250000_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "c = collections.Counter(text)\n",
    "sorted_letter_count = c.most_common() \n",
    "train_word_list = np.array(text.split('\\n'), dtype = str)[:227200]\n",
    "train_len = np.array([len(itr) for itr in train_word_list])\n",
    "\n",
    "print(len(train_word_list), train_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5512,
     "status": "ok",
     "timestamp": 1712946649986,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "DTh0nw8dhIWZ",
    "outputId": "c0618245-0da4-4584-8f96-7a6e6d6c746e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 227200/227200 [00:09<00:00, 22922.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([227200, 30])\n",
      "[['a' 'a' 'a' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'a' 'a' 'a' 'a' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'a' 's' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'c' 'h' 'e' 'n' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'e' 'e' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'g' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'h' 'e' 'd' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'h' 's' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'l' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'l' 'e' 's' 'u' 'n' 'd' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']]\n",
      "[3 6 4 6 4 3 5 4 3 8]\n",
      "1 29\n",
      "['c' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ']\n",
      "['c' 'y' 'c' 'l' 'o' 't' 'r' 'i' 'm' 'e' 't' 'h' 'y' 'l' 'e' 'n' 'e' 't'\n",
      " 'r' 'i' 'n' 'i' 't' 'r' 'a' 'm' 'i' 'n' 'e' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ']\n",
      "Vocabulary size: 29\n",
      "[' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ',']\n",
      " _abcdefghijklmnopqrstuvwxyz,\n"
     ]
    }
   ],
   "source": [
    "letter_notpresent = []\n",
    "for itr in tqdm(train_word_list):\n",
    "\n",
    "    temp = np.array([list(np.arange(26))]*len(itr)).T\n",
    "    to_sub = np.array([ord(itr1)-97 for itr1 in itr])\n",
    "    mask = temp - to_sub[None, :]\n",
    "    mask = np.prod(mask, axis = 1)\n",
    "    mask = mask != 0\n",
    "    \n",
    "    letter_notpresent.append(np.arange(26)[mask])\n",
    "    \n",
    "\n",
    "train_word_list = np.array([ list(w.ljust(40)) for w in train_word_list])\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "len_mask = torch.tensor([np.concatenate((np.zeros(train_len[itr]), np.ones(30-train_len[itr]))) for itr in range(len(train_len))])\n",
    "print(len_mask.shape)\n",
    "\n",
    "print(train_word_list[:10])\n",
    "print(train_len[:10])\n",
    "print(min(train_len), max(train_len))\n",
    "print(train_word_list[np.argmin(train_len)])\n",
    "print(train_word_list[np.argmax(train_len)])\n",
    "# get the vocabulary\n",
    "chars = sorted(list(set(text))) # vocab\n",
    "chars[0]=' '\n",
    "chars.insert(1, '_')\n",
    "chars.insert(len(chars), ',')\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# print vocab\n",
    "print(chars)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712946650132,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "RBiaUGi4MeVP",
    "outputId": "bb70f03c-8868-47c8-eba4-8d9ab4b62b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
      "[['a' 'a' 'a' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'a' 'a' 'a' 'a' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'a' 's' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'c' 'h' 'e' 'n' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']\n",
      " ['a' 'a' 'e' 'e' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(len_mask[:5])\n",
    "print(train_word_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z9eM6cPfOE-"
   },
   "source": [
    "Vocabulary is the set of unique characters / words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1712946777500,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "0jvFd2j5asFw",
    "outputId": "a49ec054-b774-4f94-ef50-da312f11a52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 1, 21, 9, 6, 19, 6, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "hi_there\n",
      "{' ': 0, '_': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, ',': 28}\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers and vice versa\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)} # abbrev for string to integer\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "# encoder: takes a string and returns a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# decoder: takes a list of integers and returns a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "#unit test\n",
    "print(encode('hi_there,'.ljust(40)))\n",
    "print(decode(encode('hi_there')))\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkEtgHZqjP9l"
   },
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1712947295186,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "XF2FG4BHtxz3",
    "outputId": "32468c2a-1b7e-49e9-eeb8-ef0a172ac3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:  0\n",
      "aag                                     \n",
      "aaas                                    \n",
      "aahs                                    \n",
      "aaaaaa                                  \n",
      "aaee                                    \n",
      "aahed                                   \n",
      "aaa                                     \n",
      "aachen                                  \n",
      "aal                                     \n",
      "aalesund                                \n",
      "aag                                     \n",
      "aal                                     \n",
      "aaa                                     \n",
      "aahed                                   \n",
      "aaee                                    \n",
      "aachen                                  \n",
      "aaas                                    \n",
      "aahs                                    \n",
      "aaaaaa                                  \n",
      "aalesund                                \n",
      "Epoch Number:  1\n",
      "aachen                                  \n",
      "aal                                     \n",
      "aaa                                     \n",
      "aaas                                    \n",
      "aahs                                    \n",
      "aahed                                   \n",
      "aaee                                    \n",
      "aaaaaa                                  \n",
      "aalesund                                \n",
      "aag                                     \n",
      "aahs                                    \n",
      "aachen                                  \n",
      "aahed                                   \n",
      "aaa                                     \n",
      "aalesund                                \n",
      "aaas                                    \n",
      "aag                                     \n",
      "aaaaaa                                  \n",
      "aal                                     \n",
      "aaee                                    \n"
     ]
    }
   ],
   "source": [
    "def get_batch_new(train_word_list, ix, train_len, avg_num_letters = 7, max_wrong_guesses = 6):\n",
    "    # create a batch using the list of indices, ix\n",
    "    batch_size = len(ix)\n",
    "    letter_array = np.arange(26)\n",
    "    p = avg_num_letters/26\n",
    "    temp_select = np.random.random((26, batch_size))\n",
    "\n",
    "    y = torch.zeros((batch_size, 40), dtype = torch.int)\n",
    "    x = torch.zeros((batch_size, 40), dtype = torch.int)\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    for itr in range(batch_size):\n",
    "        y[itr] = torch.tensor(encode(train_word_list[ix[itr]]))\n",
    "        x[itr] = y[itr]\n",
    "        letter_select = torch.tensor(letter_array[temp_select[:, itr] < p] + 2)\n",
    "        letter_unselect = torch.tensor(np.random.choice(letter_notpresent[ix[itr]] + 2,\n",
    "                                                        size =  min(26 - len(letter_select), np.random.randint(0, max_wrong_guesses)), replace = False))\n",
    "        mask = torch.isin(x[itr], letter_select)\n",
    "        x[itr] = x[itr].masked_fill(mask, 1)\n",
    "        len_curr = train_len[ix[itr]]\n",
    "        x[itr][len_curr] = 28\n",
    "        for itr1, letter in enumerate(letter_unselect):\n",
    "            if itr1+len_curr + 1 >= 40: break\n",
    "            x[itr][itr1 + len_curr + 1] = letter\n",
    "            \n",
    "\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "def batch_iterator(train_word_list, train_len, batch_size, avg_num_letters, max_wrong_guesses):\n",
    "    n = len(train_word_list)\n",
    "    indices = np.random.permutation(n)  # shuffle once per epoch\n",
    "    flag = False\n",
    "    if avg_num_letters==-1:\n",
    "        flag = True\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = indices[start:end]\n",
    "        if flag:\n",
    "            avg_num_letters = random.randint(7, 15)\n",
    "        X, y = get_batch_new(train_word_list, batch_idx, train_len, avg_num_letters, max_wrong_guesses)\n",
    "        yield X, y\n",
    "\n",
    "\n",
    "\n",
    "#This piece of code checks if the batch iterator is working \n",
    "\n",
    "for epoch in range(2):\n",
    "    print('Epoch Number: ', epoch)\n",
    "    for i in range(2):\n",
    "        for X, y in  batch_iterator(train_word_list[:10], train_len[:10], 5, 3, 10):\n",
    "            for j in range(5):\n",
    "                print(decode(list(y[j].cpu().detach().numpy())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux_WEmA8vJru"
   },
   "source": [
    "### Part 1: Define a decoder only head to perform scaled dot product attention.\n",
    "\n",
    "Consider an input $x \\in \\mathbb{R}^{B \\times T \\times C}$ to the attention head, where $B$ is the batch size, $T$ is the context length and $C$ is the embedding dimension (input dimension from the last layer).\n",
    "\n",
    "Then, the attention head of size $d$ outputs $z \\in \\mathbb{R}^{B \\times T \\times d}$\n",
    "\n",
    "$$ z = softmax \\left( \\frac{Q K^T}{\\sqrt{d}} \\right) V $$\n",
    "\n",
    "where $K, Q, V$ are linear (and not affine) transformations of the input\n",
    "\n",
    "$$ K = k x, \\text{ with } k \\in \\mathbb{R}^{d \\times C} $$\n",
    "$$ Q = q x, \\text{ with } q \\in \\mathbb{R}^{d \\times C} $$\n",
    "$$ V = v x, \\text{ with } v \\in \\mathbb{R}^{d \\times C} $$\n",
    "\n",
    "Define a nn.Module class to implement a decoder only head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712947295712,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "alslmQoR5x3n"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Description: decoder only scaled dot product attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Initializes various layers used in the forward function\n",
    "            Note: runs when an object of Head is created\n",
    "        Input:\n",
    "            n_embd (C): The embedding dimension\n",
    "            n_head (d): Hidden dimension of the single head, i.e., key.shape = query.shape = value.shape = (n_embd, head_size)\n",
    "        \"\"\"\n",
    "        # use linear layers to define key, query and value matrices\n",
    "        # bias is usually not used in nn.Linear in the attention blocks; use bias = False\n",
    "        self.key = nn.Linear(n_embd, n_head, bias = False, device = device)\n",
    "        self.query = nn.Linear(n_embd, n_head, bias = False, device = device)\n",
    "        self.value = nn.Linear(n_embd, n_head, bias = False, device = device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Forward pass of the attention head\n",
    "        Input:\n",
    "            x of shape (B, T, C)\n",
    "        Output:\n",
    "            z of shape (B, T, d)\n",
    "        \"\"\"\n",
    "        x=x.to(device)\n",
    "        B, T, C = x.shape # B: batch size; T: block_size; C: embedding dim\n",
    "        K = self.key(x)    # (B, T, d)\n",
    "        Q = self.query(x)  # (B, T, d)\n",
    "        # compute scaled dot product attention scores w = q @ k / sqrt(d)\n",
    "        W = torch.einsum('btd,bud->btu', Q, K) # (B, T, d) @ (B, T, d) -> (B, T, T)\n",
    "        # Mask out the attention scores such that the padded values are ignored\n",
    "        mask = torch.ones((B, T), device = device).masked_fill(torch.sum(x, axis = -1)==0, float('-inf')) # (B, T, T)\n",
    "        \n",
    "        # apply a softmax along the last dim;\n",
    "        # use nn.functional.softmax;\n",
    "        # Note: nn.functional is imported as F\n",
    "        W = F.softmax(W + mask[:, None, :], dim=2) # (B, T, T)\n",
    "#         print(W[:, 0,:])\n",
    "        # perform the weighted aggregation of the values: V = v @ x\n",
    "        V = self.value(x) # (B, T, d)\n",
    "        # output w @ v\n",
    "        out = W @ V # (B, T, T) @ (B, T, d) -> (B, T, d)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712947295871,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "_EHPNBOiseHd",
    "outputId": "18aaaf42-be4e-47d9-d432-0eff9a0f3fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2, 13, 10, 10, 20, 28,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  2,  1,  6, 15, 28, 18, 24, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]], device='mps:0', dtype=torch.int32)\n",
      "tensor([[ 2,  2, 13, 10, 10, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  2, 19,  6, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]], device='mps:0', dtype=torch.int32)\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 0,  0, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]], device='mps:0', dtype=torch.int32)\n",
      "tensor([[ 2,  2, 13, 10, 10, 20, 28,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 2,  2,  1,  6, 15, 28, 18, 24, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]], device='mps:0', dtype=torch.int32)\n",
      "torch.Size([2, 40, 56])\n",
      "['a' 'a' 'l' 'i' 'i' 's' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' '] ['a' 'a' 'r' 'e' 'n' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ']\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch_new(train_word_list, [10, 20], train_len, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(y.masked_fill(x>1, 0))\n",
    "print(x)\n",
    "test_embd = nn.Embedding(28, 28, padding_idx=0)\n",
    "test_embd = test_embd.to(device)\n",
    "x = test_embd(x).to(device)\n",
    "head = Head(28, 56)\n",
    "out = head(x)\n",
    "print(out.shape)\n",
    "print(train_word_list[10], train_word_list[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU_uqNKJ7puM"
   },
   "source": [
    "### Part 2: Define multi-head attention module\n",
    "\n",
    "Given input $x \\in \\mathbb{R}^{B \\times T \\times C}$ as before, define multiple heads of attention that work in parallel. Use class Head() defined above to create a list of attention heads using nn.ModuleList()\n",
    "\n",
    "Each attention head returns $z \\in \\mathbb{R}^{B \\times T \\times d}$\n",
    "\n",
    "Using a linear layer, project the concatentated output of attention heads back to the embedding dimension $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712947296350,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "b_iq79lz-vkI"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size, embd_size):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Description: Performs multi-head attention followed by a projection\n",
    "        Input:\n",
    "            num_heads: number of attention heads\n",
    "            head_size (d): size of each attention head\n",
    "        \"\"\"\n",
    "        self.heads = nn.ModuleList([Head(embd_size, head_size) for i in range(num_heads)]) # Define n_heads copies of Head() of size head_size as a list\n",
    "        self.proj = nn.Linear(num_heads*head_size, embd_size, bias = False) # projection layer using nn.Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\n",
    "        Description: Forward pass of multi head attention\n",
    "        Input: x of shape (B, T, C)\n",
    "        \"\"\"\n",
    "        multihead_out = []\n",
    "        for i, l in enumerate(self.heads):\n",
    "            multihead_out.append(l(x))\n",
    "        out = torch.cat(multihead_out, dim = 2)\n",
    "\n",
    "        #project the output using a linear\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLn3mLHu_00W"
   },
   "source": [
    "### Part 3: Implement a simple two layer ReLU FCN $f: \\mathbb{R}^n \\to \\mathbb{R}^n$ with hidden layer dimension $h = 4 \\times n$ .\n",
    "\n",
    "The network should perform the following operations:\n",
    "\n",
    "1. Linear: $\\mathbb{R}^{n \\to 4n}$\n",
    "2. ReLU: $\\mathbb{R}^{4n \\to 4n}$\n",
    "3. Linear: $\\mathbb{R}^{n \\to n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1712947297184,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "FpKaDs1dAyeH"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" A simple one hidden layer ReLU block followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.layer2 = nn.Linear(4*n_embd, n_embd)\n",
    "        return\n",
    "        \"\"\"\n",
    "        Description: Linear -> ReLU -> Linear\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Forward pass of the network\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZuvMdc_DaPk"
   },
   "source": [
    "### Part 4: Implement a Transformer block consisting of multi-head attention followed by feedforward computation\n",
    "\n",
    "Using the MultiHeadAttention() and Feedforward() classes, define a Transformer block to perform:\n",
    "\n",
    "1. LayerNorm in the embedding dimension using nn.LayerNorm(n_embd)\n",
    "2. Multi-head attention with n_head heads of size head_size\n",
    "3. Add Residual from input\n",
    "4. LayerNorm in the embedding dimension using nn.LayerNorm(n_embd)\n",
    "5. Feedforward layer: Linear -> ReLU -> Linear\n",
    "6. Add Residual from Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712947297184,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "QU4jHDtZGBFG"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: Multi head attention followed by feed forward followed by LayerNorm \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, head_size = 100):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Description: Transformer block\n",
    "        Input:\n",
    "            n_embd: embedding dimension of the input\n",
    "            n_head: number of attention heads\n",
    "        \"\"\"\n",
    "        head_size = n_embd // n_head\n",
    "        self.L1 = nn.LayerNorm(n_embd)\n",
    "        self.L2 = MultiHeadAttention(n_head, head_size, n_embd)\n",
    "        self.L3 = FeedFoward(n_embd)\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Forward pass of the attention block\"\n",
    "        residual = x\n",
    "        x = self.L2(x)\n",
    "        x = x + residual\n",
    "        residual = x\n",
    "        x = self.L3(x)\n",
    "        x = x + residual\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDU1RU7bGCTd"
   },
   "source": [
    "### Part 5: The full (decoder only) Transformer\n",
    "\n",
    "Finally, lets put all of it together to construct the full decoder only transformer. Consider the tokenized input $x \\in \\mathbb{R}^{B \\times T \\times V}$, where $B$ is the batch size, $T$ is the context length and $V$ is the vocabulary size.\n",
    "\n",
    "The model should perform the following computations:\n",
    "\n",
    "1. T = Token embedding(x) $ \\in \\mathbb{R}^{B \\times T \\times C}$, where $C$ is the embedding dimension\n",
    "2. P = Positional embedding(x) $\\in \\mathbb{R}^{T \\times C}$ .For simplicity, we will use the index of the sequence as the positional embedding.\n",
    "3. X = T + P  $ \\in \\mathbb{R}^{B \\times T \\times C}$\n",
    "4. n_layers of Transformer blocks using the Block() module.\n",
    "Use nn.Sequential() for stacking mulitple layers\n",
    "5. LayerNorm in the embeding dimension\n",
    "6. Linear transformation: $\\mathbb{R}^{C} \\to \\mathbb{R}^{V}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "def get_rotary_embedding(dim, seq_len, device):\n",
    "    \"\"\"Computes rotary position embeddings using torch built-in functions.\"\"\"\n",
    "    theta = 10000.0 ** (-torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim)\n",
    "    seq_idx = torch.arange(seq_len, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "    emb = torch.matmul(seq_idx, theta.unsqueeze(0))\n",
    "    return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "\n",
    "def apply_rotary_embedding(x, rotary_emb):\n",
    "    \"\"\"Applies rotary embedding using PyTorch operations.\"\"\"\n",
    "    x_reshaped = x.view(*x.shape[:-1], x.shape[-1] // 2, 2)\n",
    "    cos_emb, sin_emb = rotary_emb.chunk(2, dim=-1)\n",
    "    x1, x2 = x_reshaped.unbind(-1)\n",
    "    x_rot = torch.stack((x1 * cos_emb - x2 * sin_emb, x1 * sin_emb + x2 * cos_emb), dim=-1)\n",
    "    return x_rot.flatten(-2)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "dim = 64  # Embedding dimension\n",
    "seq_len = 128  # Sequence length\n",
    "batch_size = 2  # Example batch size\n",
    "\n",
    "# Generate random input tensor\n",
    "x = torch.randn(batch_size, seq_len, dim, device=device)\n",
    "\n",
    "# Compute rotary embeddings\n",
    "rotary_emb = get_rotary_embedding(dim, seq_len, device)\n",
    "\n",
    "# Apply rotary embeddings\n",
    "x_rotated = apply_rotary_embedding(x, rotary_emb)\n",
    "\n",
    "print(x_rotated.shape)  # Should be (batch_size, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712947297408,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "gLmIfjxjkv-C"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description: Decoder only transformer model\"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_layers, n_head):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Description: Decoder only transformer model\n",
    "        Input:\n",
    "            vocab_size (V) : Vocabulary dimension\n",
    "            block_size (T): Context length\n",
    "            n_embd (C): Embedding dimension\n",
    "            n_layers: number of layers of Transformer blocks\n",
    "            n_head : number of heads in multi-head attention\n",
    "        \"\"\"\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, padding_idx = 0) \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for itre in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embd) # Layer norm in the embedding dimension\n",
    "        self.linear_head = nn.Linear(n_embd, vocab_size) # (C -> V)\n",
    "\n",
    "    ### DO NOT MODIFY BEYOND THIS\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Description: Forward pass of transformer\n",
    "        Inputs:\n",
    "            idx: The tokenized input sequence\n",
    "            targets (optional): the tokenized target sequence\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        _, _, C = tok_emb.shape\n",
    "        rotary_emb = get_rotary_embedding(C, block_size, device)\n",
    "        x = apply_rotary_embedding(tok_emb, rotary_emb).masked_fill(tok_emb==0, 0)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.layer_norm(x) # (B,T,C)\n",
    "        logits = self.linear_head(x) # (B,T, V)\n",
    "\n",
    "        # computes loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.masked_fill(idx > 1 , 0)\n",
    "            targets = targets.view(B*T).type(torch.LongTensor).to(device)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets, ignore_index = 0)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_JZnK8xBMOi"
   },
   "source": [
    "Estimates training and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712947297408,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "FWckcLKllDVU"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS\n",
    "# Estimates loss\n",
    "def estimate_loss(frac = 0.27):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(train_word_list, train_len, batch_size, frac)\n",
    "        logits, loss =  model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    mean_loss = losses.mean()\n",
    "    model.train()\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1712947298700,
     "user": {
      "displayName": "Sagar Airen",
      "userId": "07378539338751765875"
     },
     "user_tz": 240
    },
    "id": "LwyIWxptBKWd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "block_size = 40 # maximum context length\n",
    "n_embd = 127\n",
    "n_head = 16\n",
    "n_layers = 5\n",
    "dropout = 0.0\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995229 M parameters\n"
     ]
    }
   ],
   "source": [
    "def printModel(model):\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "torch.manual_seed(1090)\n",
    "model = Transformer(vocab_size, block_size, n_embd+1, n_layers, n_head)\n",
    "model = model.to(device)\n",
    "# printModel(model)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 5.97 GB, other allocations: 825.25 MB, max allowed: 6.77 GB). Tried to allocate 31.25 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m niter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m  batch_iterator(train_word_list, train_len, batch_size, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     20\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m     niter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 33\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     31\u001b[0m rotary_emb \u001b[38;5;241m=\u001b[39m get_rotary_embedding(C, block_size, device)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m apply_rotary_embedding(tok_emb, rotary_emb)\u001b[38;5;241m.\u001b[39mmasked_fill(tok_emb\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_head(x) \u001b[38;5;66;03m# (B,T, V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward pass of the attention block\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL2(x)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m residual\n\u001b[1;32m     24\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads):\n\u001b[1;32m     21\u001b[0m     multihead_out\u001b[38;5;241m.\u001b[39mappend(l(x))\n\u001b[0;32m---> 22\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(multihead_out, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#project the output using a linear\u001b[39;00m\n\u001b[1;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 5.97 GB, other allocations: 825.25 MB, max allowed: 6.77 GB). Tried to allocate 31.25 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 800\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item()\n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/20 - Avg batch loss: 2.0212 - Full train loss: 2.0169\n",
      "Epoch 32/20 - Avg batch loss: 2.0185 - Full train loss: 2.0152\n",
      "Epoch 33/20 - Avg batch loss: 2.0166 - Full train loss: 2.0115\n",
      "Epoch 34/20 - Avg batch loss: 2.0116 - Full train loss: 2.0114\n",
      "Epoch 35/20 - Avg batch loss: 2.0097 - Full train loss: 2.0075\n",
      "Epoch 36/20 - Avg batch loss: 2.0057 - Full train loss: 2.0029\n",
      "Epoch 37/20 - Avg batch loss: 2.0040 - Full train loss: 2.0038\n",
      "Epoch 38/20 - Avg batch loss: 2.0010 - Full train loss: 1.9992\n",
      "Epoch 39/20 - Avg batch loss: 1.9988 - Full train loss: 1.9973\n",
      "Epoch 40/20 - Avg batch loss: 1.9955 - Full train loss: 1.9984\n",
      "Epoch 41/20 - Avg batch loss: 1.9937 - Full train loss: 1.9912\n",
      "Epoch 42/20 - Avg batch loss: 1.9927 - Full train loss: 1.9883\n",
      "Epoch 43/20 - Avg batch loss: 1.9895 - Full train loss: 1.9863\n",
      "Epoch 44/20 - Avg batch loss: 1.9878 - Full train loss: 1.9849\n",
      "Epoch 45/20 - Avg batch loss: 1.9874 - Full train loss: 1.9840\n",
      "Epoch 46/20 - Avg batch loss: 1.9838 - Full train loss: 1.9823\n",
      "Epoch 47/20 - Avg batch loss: 1.9834 - Full train loss: 1.9818\n",
      "Epoch 48/20 - Avg batch loss: 1.9788 - Full train loss: 1.9788\n",
      "Epoch 49/20 - Avg batch loss: 1.9753 - Full train loss: 1.9767\n",
      "Epoch 50/20 - Avg batch loss: 1.9764 - Full train loss: 1.9744\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(30, 30 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item()\n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/20 - Avg batch loss: 1.6564 - Full train loss: 1.9644\n",
      "Epoch 52/20 - Avg batch loss: 1.6526 - Full train loss: 1.9631\n",
      "Epoch 53/20 - Avg batch loss: 1.6487 - Full train loss: 1.9603\n",
      "Epoch 54/20 - Avg batch loss: 1.6471 - Full train loss: 1.9626\n",
      "Epoch 55/20 - Avg batch loss: 1.6450 - Full train loss: 1.9604\n",
      "Epoch 56/20 - Avg batch loss: 1.6444 - Full train loss: 1.9602\n",
      "Epoch 57/20 - Avg batch loss: 1.6427 - Full train loss: 1.9583\n",
      "Epoch 58/20 - Avg batch loss: 1.6411 - Full train loss: 1.9576\n",
      "Epoch 59/20 - Avg batch loss: 1.6381 - Full train loss: 1.9578\n",
      "Epoch 60/20 - Avg batch loss: 1.6405 - Full train loss: 1.9576\n",
      "Epoch 61/20 - Avg batch loss: 1.6392 - Full train loss: 1.9566\n",
      "Epoch 62/20 - Avg batch loss: 1.6360 - Full train loss: 1.9542\n",
      "Epoch 63/20 - Avg batch loss: 1.6339 - Full train loss: 1.9566\n",
      "Epoch 64/20 - Avg batch loss: 1.6358 - Full train loss: 1.9531\n",
      "Epoch 65/20 - Avg batch loss: 1.6340 - Full train loss: 1.9538\n",
      "Epoch 66/20 - Avg batch loss: 1.6297 - Full train loss: 1.9536\n",
      "Epoch 67/20 - Avg batch loss: 1.6308 - Full train loss: 1.9533\n",
      "Epoch 68/20 - Avg batch loss: 1.6282 - Full train loss: 1.9505\n",
      "Epoch 69/20 - Avg batch loss: 1.6287 - Full train loss: 1.9497\n",
      "Epoch 70/20 - Avg batch loss: 1.6297 - Full train loss: 1.9502\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(50, 50 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 10, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item()\n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('model_127_16_5_epoch_70')\n",
    "model = Transformer(vocab_size, block_size, n_embd+1, n_layers, n_head)\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/20 - Avg batch loss: 1.5024 - Full train loss: 1.9533\n",
      "Epoch 72/20 - Avg batch loss: 1.5041 - Full train loss: 1.9565\n",
      "Epoch 73/20 - Avg batch loss: 1.5041 - Full train loss: 1.9550\n",
      "Epoch 74/20 - Avg batch loss: 1.5031 - Full train loss: 1.9525\n",
      "Epoch 75/20 - Avg batch loss: 1.4986 - Full train loss: 1.9544\n",
      "Epoch 76/20 - Avg batch loss: 1.4978 - Full train loss: 1.9515\n",
      "Epoch 77/20 - Avg batch loss: 1.4978 - Full train loss: 1.9521\n",
      "Epoch 78/20 - Avg batch loss: 1.4986 - Full train loss: 1.9495\n",
      "Epoch 79/20 - Avg batch loss: 1.4950 - Full train loss: 1.9511\n",
      "Epoch 80/20 - Avg batch loss: 1.4960 - Full train loss: 1.9518\n",
      "Epoch 81/20 - Avg batch loss: 1.4942 - Full train loss: 1.9507\n",
      "Epoch 82/20 - Avg batch loss: 1.4880 - Full train loss: 1.9514\n",
      "Epoch 83/20 - Avg batch loss: 1.4918 - Full train loss: 1.9502\n",
      "Epoch 84/20 - Avg batch loss: 1.4902 - Full train loss: 1.9485\n",
      "Epoch 85/20 - Avg batch loss: 1.4895 - Full train loss: 1.9503\n",
      "Epoch 86/20 - Avg batch loss: 1.4885 - Full train loss: 1.9530\n",
      "Epoch 87/20 - Avg batch loss: 1.4894 - Full train loss: 1.9510\n",
      "Epoch 88/20 - Avg batch loss: 1.4848 - Full train loss: 1.9506\n",
      "Epoch 89/20 - Avg batch loss: 1.4854 - Full train loss: 1.9486\n",
      "Epoch 90/20 - Avg batch loss: 1.4858 - Full train loss: 1.9484\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(70, 70 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 8, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item()\n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/20 - Avg batch loss: 1.4779 - Full train loss: 1.9424\n",
      "Epoch 92/20 - Avg batch loss: 1.4769 - Full train loss: 1.9454\n",
      "Epoch 93/20 - Avg batch loss: 1.4791 - Full train loss: 1.9431\n",
      "Epoch 94/20 - Avg batch loss: 1.4753 - Full train loss: 1.9431\n",
      "Epoch 95/20 - Avg batch loss: 1.4770 - Full train loss: 1.9417\n",
      "Epoch 96/20 - Avg batch loss: 1.4758 - Full train loss: 1.9425\n",
      "Epoch 97/20 - Avg batch loss: 1.4734 - Full train loss: 1.9409\n",
      "Epoch 98/20 - Avg batch loss: 1.4728 - Full train loss: 1.9414\n",
      "Epoch 99/20 - Avg batch loss: 1.4752 - Full train loss: 1.9390\n",
      "Epoch 100/20 - Avg batch loss: 1.4761 - Full train loss: 1.9407\n",
      "Epoch 101/20 - Avg batch loss: 1.4750 - Full train loss: 1.9399\n",
      "Epoch 102/20 - Avg batch loss: 1.4734 - Full train loss: 1.9415\n",
      "Epoch 103/20 - Avg batch loss: 1.4716 - Full train loss: 1.9411\n",
      "Epoch 104/20 - Avg batch loss: 1.4729 - Full train loss: 1.9426\n",
      "Epoch 105/20 - Avg batch loss: 1.4726 - Full train loss: 1.9421\n",
      "Epoch 106/20 - Avg batch loss: 1.4710 - Full train loss: 1.9384\n",
      "Epoch 107/20 - Avg batch loss: 1.4711 - Full train loss: 1.9420\n",
      "Epoch 108/20 - Avg batch loss: 1.4717 - Full train loss: 1.9397\n",
      "Epoch 109/20 - Avg batch loss: 1.4706 - Full train loss: 1.9395\n",
      "Epoch 110/20 - Avg batch loss: 1.4710 - Full train loss: 1.9397\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(90, 90 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 8, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item() \n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/20 - Avg batch loss: 1.5991 - Full train loss: 1.9378\n",
      "Epoch 112/20 - Avg batch loss: 1.5993 - Full train loss: 1.9358\n",
      "Epoch 113/20 - Avg batch loss: 1.6001 - Full train loss: 1.9364\n",
      "Epoch 114/20 - Avg batch loss: 1.5974 - Full train loss: 1.9343\n",
      "Epoch 115/20 - Avg batch loss: 1.5988 - Full train loss: 1.9334\n",
      "Epoch 116/20 - Avg batch loss: 1.5981 - Full train loss: 1.9358\n",
      "Epoch 117/20 - Avg batch loss: 1.5985 - Full train loss: 1.9346\n",
      "Epoch 118/20 - Avg batch loss: 1.5962 - Full train loss: 1.9339\n",
      "Epoch 119/20 - Avg batch loss: 1.5964 - Full train loss: 1.9346\n",
      "Epoch 120/20 - Avg batch loss: 1.5952 - Full train loss: 1.9327\n",
      "Epoch 121/20 - Avg batch loss: 1.5965 - Full train loss: 1.9349\n",
      "Epoch 122/20 - Avg batch loss: 1.5955 - Full train loss: 1.9309\n",
      "Epoch 123/20 - Avg batch loss: 1.5965 - Full train loss: 1.9320\n",
      "Epoch 124/20 - Avg batch loss: 1.5942 - Full train loss: 1.9328\n",
      "Epoch 125/20 - Avg batch loss: 1.5950 - Full train loss: 1.9317\n",
      "Epoch 126/20 - Avg batch loss: 1.5949 - Full train loss: 1.9327\n",
      "Epoch 127/20 - Avg batch loss: 1.5962 - Full train loss: 1.9299\n",
      "Epoch 128/20 - Avg batch loss: 1.5949 - Full train loss: 1.9301\n",
      "Epoch 129/20 - Avg batch loss: 1.5910 - Full train loss: 1.9310\n",
      "Epoch 130/20 - Avg batch loss: 1.5948 - Full train loss: 1.9327\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(110, 110 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 10, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item() \n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('model_127_16_5_epoch_130')\n",
    "model = Transformer(vocab_size, block_size, n_embd+1, n_layers, n_head)\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/20 - Avg batch loss: 1.4653 - Full train loss: 1.9348\n",
      "Epoch 132/20 - Avg batch loss: 1.4610 - Full train loss: 1.9350\n",
      "Epoch 133/20 - Avg batch loss: 1.4641 - Full train loss: 1.9380\n",
      "Epoch 134/20 - Avg batch loss: 1.4625 - Full train loss: 1.9334\n",
      "Epoch 135/20 - Avg batch loss: 1.4631 - Full train loss: 1.9357\n",
      "Epoch 136/20 - Avg batch loss: 1.4649 - Full train loss: 1.9319\n",
      "Epoch 137/20 - Avg batch loss: 1.4608 - Full train loss: 1.9327\n",
      "Epoch 138/20 - Avg batch loss: 1.4609 - Full train loss: 1.9338\n",
      "Epoch 139/20 - Avg batch loss: 1.4639 - Full train loss: 1.9336\n",
      "Epoch 140/20 - Avg batch loss: 1.4632 - Full train loss: 1.9336\n",
      "Epoch 141/20 - Avg batch loss: 1.4609 - Full train loss: 1.9356\n",
      "Epoch 142/20 - Avg batch loss: 1.4609 - Full train loss: 1.9358\n",
      "Epoch 143/20 - Avg batch loss: 1.4637 - Full train loss: 1.9325\n",
      "Epoch 144/20 - Avg batch loss: 1.4610 - Full train loss: 1.9348\n",
      "Epoch 145/20 - Avg batch loss: 1.4593 - Full train loss: 1.9325\n",
      "Epoch 146/20 - Avg batch loss: 1.4613 - Full train loss: 1.9331\n",
      "Epoch 147/20 - Avg batch loss: 1.4570 - Full train loss: 1.9344\n",
      "Epoch 148/20 - Avg batch loss: 1.4577 - Full train loss: 1.9329\n",
      "Epoch 149/20 - Avg batch loss: 1.4612 - Full train loss: 1.9325\n",
      "Epoch 150/20 - Avg batch loss: 1.4587 - Full train loss: 1.9331\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 400\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "for epoch in range(130, 130 + num_epochs):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 8, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    # After every epoch see the loss on training set\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    niter = 0\n",
    "    for xb, yb in  batch_iterator(train_word_list, train_len, batch_size, 15, 6):\n",
    "        logits, loss = model(xb, yb)\n",
    "        train_loss += loss.item() \n",
    "        niter += 1\n",
    "    train_loss = train_loss/niter\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Avg batch loss: {np.mean(epoch_losses):.4f} - \"\n",
    "          f\"Full train loss: {train_loss:.4f}\")\n",
    "\n",
    "    if (epoch+1)%5==0:\n",
    "        torch.save(model.state_dict(), f'model_{n_embd}_{n_head}_{n_layers}_epoch_{epoch+1}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
